ğŸ“¦ ML Pipeline for Point Cloud Semantic Segmentation

This project implements the initial version of a scalable ML pipeline for semantic segmentation on outdoor LiDAR point clouds.
It includes:

âœ” Commercial-use-permitted dataset (PandaSet)

âœ” Python-based dataset downloader

âœ” Exploratory Data Analysis (EDA)

âœ” Preprocessing (voxel downsampling, sampling, augmentation)

âœ” PyTorch Dataset for PointNext

âœ” Model selection (PointNeXt)

âœ” Training stub (no-op)


ğŸ”§ Project Structure
.
â”œâ”€â”€ src/

â”‚   â”œâ”€â”€ download.py          # Dataset downloader

â”‚   â”œâ”€â”€ preprocess.py        # Filtering, voxelization, sampling, augmentation

â”‚   â”œâ”€â”€ datasets.py          # PointNeXt-compatible PyTorch Dataset

â”‚   â”œâ”€â”€ utils.py             # Helper functions

â”‚   â”œâ”€â”€ EDA.py               # Visualization & stats

â”œâ”€â”€ pipeline.ipynb           # Full pipeline execution 

â”œâ”€â”€ MODEL_CHOICE.md          # Explain model choice

â”œâ”€â”€ Illustrations            # Stores visualisations of pcds

â”‚     

â”œâ”€â”€ class.json               # Class mapping from dataset 

â”œâ”€â”€ requirements.txt

â”œâ”€â”€ README.md

â””â”€â”€ Licensing.md             # Explain the license of the DataSet/Libraries used


ğŸš€ 1. Clone this Repository

git clone https://github.com/sleepybrain999/Point-Cloud-Semantic-Segmentation.git

cd Point-Cloud-Semantic-Segmentation

ğŸ“¥ 2. Create a clean conda environment on python 3.11

conda create -n Your_env python=3.11

conda activate Your_env

ğŸ“¥ 3. Install Python Requirements

pip install -r requirements.txt

ğŸ“¦ 4. Install PandaSet Devkit 

git clone https://github.com/scaleapi/pandaset-devkit.git

cd pandaset-devkit/python

pip install .

ğŸ“¦ 5. Modify PandaSet Devkit 
    
    In pandaset-devkit/python/pandaset/sensors, Change:

    class Lidar(Sensor):
    @property
    def _data_file_extension(self) -> str:
        return 'pkl.gz'
    
    to:
    class Lidar(Sensor):
    @property
    def _data_file_extension(self) -> str:
        return 'pkl'

    In pandaset-devkit/python/pandaset/annotations, Change:

    class SemanticSegmentation(Annotation):
    @property
    def _data_file_extension(self) -> str:
        return 'pkl.gz'

    to:
    class SemanticSegmentation(Annotation):

    @property
    def _data_file_extension(self) -> str:
        return 'pkl'

  
ğŸ“¡ 6. Download PandaSet Dataset and run the code

Open:

pipeline.ipynb

Replace the kaggle username and api key with your own credentials

The notebook walks through:

Dataset download

EDA

Preprocessing

Dataset construction

Model selection (PointNeXt)

Training stub

functions used in this notebook can be found in either panda-devkit or src folder


ğŸ“ Attribution

PandaSet Dataset Attribution (CC-BY-ND 4.0)

This project uses the PandaSet dataset provided by Scale AI & Hesai.
PandaSet is licensed under Creative Commons Attributionâ€“NoDerivatives 4.0 International (CC-BY-ND 4.0).


PandaSet was created in collaboration between Scale AI and Hesai.
Â© 2020 Scale AI. Licensed under the Creative Commons Attributionâ€“NoDerivatives 4.0 International License (CC BY 4.0). https://creativecommons.org/licenses/by/4.0/#ref-appropriate-credit

Dataset source: https://pandaset-git-master.scaleai1.vercel.app/

PandaSet was preprocessed and used for training machine learning models


PointNeXt Model Attribution

This project references the PointNeXt architecture for model selection and configuration examples.

@InProceedings{qian2022pointnext,
  title   = {PointNeXt: Revisiting PointNet++ with Improved Training and Scaling Strategies},
  author  = {Qian, Guocheng and Li, Yuchen and Peng, Houwen and Mai, Jinjie and Hammoud, Hasan and Elhoseiny, Mohamed and Ghanem, Bernard},
  booktitle=Advances in Neural Information Processing Systems (NeurIPS),
  year    = {2022},
}

